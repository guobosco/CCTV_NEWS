# 后端服务配置
backend:
  host: "0.0.0.0"          # 服务器IP，0.0.0.0表示监听所有网卡
  port: 5001                # 端口号
  debug: true               # 调试模式

# 数据库配置
database:
  path: "news.db"           # 数据库文件路径，相对于脚本所在目录

# 爬虫配置
spider:
  backend_url: "http://localhost:5001"  # 后端服务URL
  request_timeout: 10       # 请求超时时间（秒）
  max_retries: 5            # 最大重试次数
  request_delay: 1.0        # 请求延迟（秒）
  batch_size: 10            # 批量处理大小

# 批量运行配置
batch:
  python_path: "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\python.exe"  # Python解释器路径
  main_script: "news_to_sqlite.py"  # 主爬虫脚本路径
  default_start_date: "2026-01-01"  # 默认开始日期

# 日志配置
logging:
  level: "INFO"             # 日志级别：DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_file: "scheduled_spider.log"  # 定时爬虫日志文件
  news_log_file: "news_to_sqlite.log"  # 新闻爬虫日志文件
  batch_log_file: "batch_run.log"  # 批量运行日志文件
